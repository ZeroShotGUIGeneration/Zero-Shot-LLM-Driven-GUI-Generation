{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n",
    "\n",
    "from typing import Text, Generator, Tuple, List, Optional, Dict, Set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "import copy\n",
    "sns.set_theme()\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load GUI Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "rico_path = \"../data/rico/unique_uis/combined/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_guis_with_comps = pd.read_csv(data_path + \"all_guis.csv\")\n",
    "all_guis_with_comps['data'] = all_guis_with_comps['data'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_guis_with_comps[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help function for plotting multiple retrieved images\n",
    "def show_images(ranked, img_path, relevances=None):\n",
    "    fig = plt.figure(figsize=(15,15)) # specifying the overall grid size\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(int(math.ceil(len(ranked)/3)),3),\n",
    "                 axes_pad=0.3,  share_all=True\n",
    "                 )\n",
    "    if relevances is None:\n",
    "        relevances = np.zeros(len(ranked))\n",
    "    for ax, rank, relevance in zip(grid, ranked, relevances):\n",
    "        rico_id = rank[0]\n",
    "        img = Image.open(img_path + str(rico_id) + \".jpg\")\n",
    "        img = img.resize((1080, 1920))\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.grid(False)\n",
    "        title = str(rico_id) + \", \" + str(round(rank[1], 2))\n",
    "        if relevance != 0:\n",
    "            title += \", \" + relevance\n",
    "        ax.title.set_text(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_METHOD_TEXT_ONLY = 'feat_method_text_only'\n",
    "FEAT_METHOD_TEXT_COMP_TYPE = 'feat_method_text_comp_type'\n",
    "FEAT_METHOD_TEXT_COMP_TYPE_RES_ID = 'feat_method_text_comp_type_res_id'\n",
    "FEAT_METHOD_HTML = 'feat_method_html'\n",
    "\n",
    "STRUCT_METHOD_SIMPLE_BULLETS = 'struct_method_simple_bullets'\n",
    "STRUCT_METHOD_SIMPLE_BULLETS_SORTED = 'struct_method_simple_bullets_sorted'\n",
    "STRUCT_METHOD_TWO_LEVEL_BULLETS = 'struct_method_two_level_bullets'\n",
    "STRUCT_METHOD_TWO_LEVEL_HTML = 'struct_method_two_level_html'\n",
    "\n",
    "STYLE_SIZE = 'style_size'\n",
    "STYLE_BOUNDS = 'style_bounds'\n",
    "STYLE_RICO_ID = 'style_rico_id'\n",
    "STYLE_BACK_COLOR = 'style_back_color'\n",
    "STYLE_FONT_COLOR = 'style_font_color'\n",
    "STYLE_FONT_SIZE = 'style_font_size'\n",
    "\n",
    "stop_words_r_ids = {'main', 'content', 'navigation', 'bar', 'background', 'status',\n",
    "                    'checkbox', 'widget', 'frame', 'container', 'action', 'btn', 'menu',\n",
    "                    'label', 'root', 'toolbar', 'view', 'button', 'activity', 'layout',\n",
    "                    'drawer', 'actionbar', 'icon', 'text', 'banner'}\n",
    "\n",
    "html_comp_mapping = {'Web View': ('<div', '</div>'),\n",
    "                     'Icon': ('<i class=\"material-icons\"', '</i>'),\n",
    "                     'Button': ('<button type=\"button\"', '</button>'),\n",
    "                     'Label': ('<p', '</p>'),\n",
    "                     'Video': ('<video ', '</video> '),\n",
    "                     'Image': ('<img src=\"example.jpg\"', ''),\n",
    "                     'Background Image': ('<img src=\"example.jpg\"', ''),\n",
    "                     'Text': ('<p>', '</p>'),\n",
    "                     'Checkbox': ('<input type=\"checkbox\"', '</input>'),\n",
    "                     'Switch': ('<input type=\"checkbox\"', '</input>'),\n",
    "                     'Text Input': ('<input type=\"text\"', '</input>'),\n",
    "                     'Input': ('<input type=\"text\"', '</input>'),\n",
    "                     'Advertisement': ('<div', '</div>'),\n",
    "                     'Slider': ('<input type=\"range\" min=\"1\" max=\"100\"', '</input>'),\n",
    "                     'Radio Button': ('<input type=\"radio\"', '</input>'),\n",
    "                     'Pager Indicator': ('<div', '</div>'),\n",
    "                     'Map View': ('<div', '</div>')}\n",
    "\n",
    "html_comp_group_mapping = {\n",
    "    'List Item': ('<li', '</li>'),\n",
    "    'Card': ('<div', '</div>'),\n",
    "    'Modal': ('<div class=\"modal\"', '</div>'),\n",
    "    'Map View': ('<div class=\"map\"', '</div>'),\n",
    "    'Toolbar': ('<menu', '</menu>'),\n",
    "    'Multi-Tab': ('<div class=\"tab\"', '</div>'),\n",
    "    'Layout': ('<div class=\"layout\"', '</div>')\n",
    "}\n",
    "\n",
    "def normalize_resource_id(resource_id: Text, filter_tokens: Optional[Set[Text]] = None,\n",
    "                          tokenize: Optional[bool] = False) -> List[Text]:\n",
    "    stopwords = filter_tokens if filter_tokens else stop_words_r_ids\n",
    "    name_split = resource_id.split('/')\n",
    "    name = name_split[len(name_split) - 1]\n",
    "    norm_name = [token for token in snake_camel_case_split(name) if token.lower() not in stopwords]\n",
    "    return norm_name if tokenize else ' '.join(norm_name)\n",
    "\n",
    "def camel_case_split(identifier: Text) -> List[Text]:\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def snake_case_split(identifier: Text) -> List[Text]:\n",
    "    return identifier.split('_')\n",
    "\n",
    "def snake_camel_case_split(identifier: Text) -> List[Text]:\n",
    "    snake_cases = snake_case_split(identifier)\n",
    "    splits = [cc for sc in snake_cases\n",
    "              for cc in camel_case_split(sc)]\n",
    "    return splits\n",
    "\n",
    "def get_refined_comp_type(comp):\n",
    "    if comp['componentLabel'] == 'On/Off Switch':\n",
    "        return 'Switch'\n",
    "    if comp['componentLabel'] == 'Input':\n",
    "        clazz_name = comp['class'].lower()\n",
    "        if 'edittext' in clazz_name:\n",
    "            # return 'Edit Text'\n",
    "            return 'Text Input'\n",
    "        elif 'checkbox' in clazz_name:\n",
    "            return 'Checkbox'\n",
    "        elif 'switch' in clazz_name:\n",
    "            return 'Switch'\n",
    "        else:\n",
    "            return 'Input'\n",
    "    if comp['componentLabel'] == 'Text Button':\n",
    "        clazz_name = comp['class'].lower()\n",
    "        if 'checkbox' in clazz_name:\n",
    "            return 'Checkbox'\n",
    "        else:\n",
    "            return 'Button'\n",
    "    if comp['componentLabel'] == 'Text':\n",
    "        return 'Label'\n",
    "    return comp['componentLabel']\n",
    "\n",
    "def feat_method_text_only(gui, n, m, to_lower, quote, style):\n",
    "    features = [(ui_comp['id'], ' '.join(ui_comp['text'].split(' ')[:m]))\n",
    "                for ui_comp in gui['ui_comps'] if ui_comp.get('text')][:n]\n",
    "    if to_lower:\n",
    "        features = [(feat[0], feat[1].lower()) for feat in features]\n",
    "    if quote:\n",
    "        features = [(feat[0], '\"' + feat[1] + '\"') for feat in features]\n",
    "    return {feat[0]: feat[1] for feat in features}\n",
    "\n",
    "def feat_method_text_comp_type(gui, n, m, to_lower, quote, style):\n",
    "    features = []\n",
    "    for ui_comp in gui['ui_comps']:\n",
    "        uic_text = ui_comp.get('text').strip() if ui_comp.get('text') else ui_comp.get('text_updated').strip() if ui_comp.get('text_updated') else ''\n",
    "        uic_text = '\"' + uic_text + '\"' if quote else uic_text\n",
    "        feat_str = ''\n",
    "        if ui_comp.get('componentLabel') == 'Icon':\n",
    "            icon_text = ' '.join(ui_comp.get('iconClass').split('_')).strip()\n",
    "            icon_text = '\"' + icon_text + '\"' if quote else icon_text\n",
    "            feat_str = icon_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "        elif ui_comp.get('componentLabel') == 'Text Button':\n",
    "            if ui_comp.get('buttonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('buttonClass').split('_')).strip()\n",
    "                button_text = '\"' + button_text + '\"' if quote else button_text\n",
    "                feat_str = button_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            elif ui_comp.get('textButtonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('textButtonClass').split('_')).strip()\n",
    "                button_text = '\"' + button_text + '\"' if quote else button_text\n",
    "                feat_str = button_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = uic_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "        elif ui_comp.get('componentLabel') == 'Input':\n",
    "            if ui_comp.get('text'):\n",
    "                input_text = '\"' + uic_text + '\"' if quote else button_text\n",
    "                feat_str = input_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            elif ui_comp.get('text_updated'):\n",
    "                input_text = '\"' + ui_comp.get('text_updated').strip().replace('\\n', '').replace('\\f', '') + '\"' \\\n",
    "                    if quote else ui_comp.get('text_updated').strip().replace('\\n', '').replace('\\f', '')\n",
    "                feat_str = input_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = '(' + get_refined_comp_type(ui_comp) + ')'\n",
    "        else:\n",
    "            if ui_comp.get('text'):\n",
    "                feat_str = uic_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = '(' + get_refined_comp_type(ui_comp) + ')'\n",
    "        style_attrs = []\n",
    "        if style.get(STYLE_SIZE):\n",
    "            bounds = ui_comp['bounds']\n",
    "            width, height = bounds[2] - bounds[0], bounds[3] - bounds[1]\n",
    "            style_attrs.append('width:' + str(width))\n",
    "            style_attrs.append('height:' + str(height))\n",
    "        if style.get(STYLE_BOUNDS):\n",
    "            bounds = ui_comp['bounds']\n",
    "            style_attrs.append('top_left_corner: (' + str(bounds[0]) + \", \" + str(bounds[1]) + \"), \" +\\\n",
    "                               'bottom_right_corner: (' + str(bounds[1]) + \", \" + str(bounds[3]) + \")\")\n",
    "        if style.get(STYLE_RICO_ID):\n",
    "            rico_id = ui_comp[\"id\"]\n",
    "            style_attrs.append('id: ' + str(rico_id))\n",
    "        if style.get(STYLE_BACK_COLOR) and ui_comp.get('bg_color'):\n",
    "            style_attrs.append('bg_color:' + ui_comp.get('bg_color'))\n",
    "        if style.get(STYLE_FONT_COLOR) and ui_comp.get('text_color'):\n",
    "            style_attrs.append('text_color:' + ui_comp.get('text_color'))\n",
    "        if style.get(STYLE_FONT_SIZE) and ui_comp.get('font_size'):\n",
    "            style_attrs.append('font_size:' + str(int(ui_comp.get('font_size'))))\n",
    "        if style_attrs:\n",
    "            feat_str = feat_str + ' (' + '; '.join(style_attrs) + ')'\n",
    "        if to_lower:\n",
    "            feat_str = feat_str.lower()\n",
    "        features.append((ui_comp.get('id'), feat_str, ui_comp.get('bounds')))\n",
    "    return {feat[0]: (feat[1], feat[2]) for feat in features}\n",
    "\n",
    "def feat_method_text_comp_type_res_id(gui, n, m, to_lower, quote, style, id=False):\n",
    "    features = []\n",
    "    for ui_comp in gui['ui_comps']:\n",
    "        uic_text = ui_comp.get('text').strip() if ui_comp.get('text') else ui_comp.get('text_updated').strip() if ui_comp.get('text_updated') else ''\n",
    "        uic_text = '\"' + uic_text + '\"' if quote else uic_text\n",
    "        feat_str = ''\n",
    "        if ui_comp.get('componentLabel') == 'Icon':\n",
    "            icon_text = ' '.join(ui_comp.get('iconClass').split('_')).strip()\n",
    "            icon_text = '\"' + icon_text + '\"' if quote else icon_text\n",
    "            feat_str = icon_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "        elif ui_comp.get('componentLabel') == 'Text Button':\n",
    "            if ui_comp.get('buttonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('buttonClass').split('_')).strip()\n",
    "                button_text = '\"' + button_text + '\"' if quote else button_text\n",
    "                feat_str = button_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            elif ui_comp.get('textButtonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('textButtonClass').split('_')).strip()\n",
    "                button_text = '\"' + button_text + '\"' if quote else button_text\n",
    "                feat_str = button_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = uic_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "        elif ui_comp.get('componentLabel') == 'Input':\n",
    "            if ui_comp.get('text'):\n",
    "                input_text = '\"' + uic_text + '\"' if quote else button_text\n",
    "                feat_str = input_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            elif ui_comp.get('text_updated'):\n",
    "                input_text = '\"' + ui_comp.get('text_updated').strip().replace('\\n', '').replace('\\f', '') + '\"' \\\n",
    "                    if quote else ui_comp.get('text_updated').strip().replace('\\n', '').replace('\\f', '')\n",
    "                feat_str = input_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = '(' + get_refined_comp_type(ui_comp) + ')'\n",
    "        else:\n",
    "            if ui_comp.get('text'):\n",
    "                feat_str = uic_text + ' (' + get_refined_comp_type(ui_comp) + ')'\n",
    "            else:\n",
    "                feat_str = '(' + get_refined_comp_type(ui_comp) + ')'\n",
    "        feat_str += ' (' + normalize_resource_id(ui_comp.get('resource-id')) + ')' if ui_comp.get(\n",
    "            'resource-id') else ''\n",
    "        style_attrs = []\n",
    "        if style.get(STYLE_SIZE):\n",
    "            bounds = ui_comp['bounds']\n",
    "            width, height = bounds[2] - bounds[0], bounds[3] - bounds[1]\n",
    "            style_attrs.append('width:' + str(width))\n",
    "            style_attrs.append('height:' + str(height))\n",
    "        if style.get(STYLE_BOUNDS):\n",
    "            bounds = ui_comp['bounds']\n",
    "            style_attrs.append('top_left_corner: (' + str(bounds[0]) + \", \" + str(bounds[1]) + \"), \" +\\\n",
    "                               'bottom_right_corner: (' + str(bounds[1]) + \", \" + str(bounds[3]) + \")\")\n",
    "        if style.get(STYLE_RICO_ID):\n",
    "            rico_id = ui_comp[\"id\"]\n",
    "            style_attrs.append('id: ' + str(rico_id))\n",
    "        if style.get(STYLE_BACK_COLOR) and ui_comp.get('bg_color'):\n",
    "            style_attrs.append('bg_color:' + ui_comp.get('bg_color'))\n",
    "        if style.get(STYLE_FONT_COLOR) and ui_comp.get('text_color'):\n",
    "            style_attrs.append('text_color:' + ui_comp.get('text_color'))\n",
    "        if style.get(STYLE_FONT_SIZE) and ui_comp.get('font_size'):\n",
    "            style_attrs.append('font_size:' + str(int(ui_comp.get('font_size'))))\n",
    "        if style_attrs:\n",
    "            feat_str = feat_str + ' (' + '; '.join(style_attrs) + ')'\n",
    "        if to_lower:\n",
    "            feat_str = feat_str.lower()\n",
    "        if id:\n",
    "            feat_str = feat_str + ' (id=' + ui_comp.get('id').split('_')[1] + ')'\n",
    "        features.append((ui_comp.get('id'), feat_str, ui_comp.get('bounds')))\n",
    "    return {feat[0]: (feat[1], feat[2]) for feat in features}\n",
    "\n",
    "def feat_method_html(gui, n, m, to_lower, quote, style):\n",
    "    features = []\n",
    "    for ui_comp in gui['ui_comps']:\n",
    "        uic_text = ui_comp.get('text').strip() if ui_comp.get('text') else ui_comp.get('text_updated').strip() if ui_comp.get('text_updated') else ''\n",
    "        html_comp = html_comp_mapping.get(get_refined_comp_type(ui_comp))\n",
    "        feat_str = html_comp[0]\n",
    "        if ui_comp.get('resource-id'):\n",
    "            feat_str += ' id=\"' + '-'.join(normalize_resource_id(ui_comp.get('resource-id')).split(' ')) + '\"'\n",
    "        style_attrs = []\n",
    "        if style.get(STYLE_SIZE):\n",
    "            bounds = ui_comp['bounds']\n",
    "            width, height = bounds[2] - bounds[0], bounds[3] - bounds[1]\n",
    "            style_attrs.append('width:' + str(width))\n",
    "            style_attrs.append('height:' + str(height))\n",
    "        if style.get(STYLE_BOUNDS):\n",
    "            bounds = ui_comp['bounds']\n",
    "            style_attrs.append('top_left_corner: (' + str(bounds[0]) + \", \" + str(bounds[1]) + \"), \" +\\\n",
    "                               'bottom_right_corner: (' + str(bounds[1]) + \", \" + str(bounds[3]) + \")\")\n",
    "        if style.get(STYLE_RICO_ID):\n",
    "            rico_id = ui_comp[\"id\"]\n",
    "            style_attrs.append('id: ' + str(rico_id))\n",
    "        if style.get(STYLE_BACK_COLOR) and ui_comp.get('bg_color'):\n",
    "            style_attrs.append('bg_color:' + ui_comp.get('bg_color'))\n",
    "        if style.get(STYLE_FONT_COLOR) and ui_comp.get('text_color'):\n",
    "            style_attrs.append('text_color:' + ui_comp.get('text_color'))\n",
    "        if style.get(STYLE_FONT_SIZE) and ui_comp.get('font_size'):\n",
    "            style_attrs.append('font_size:' + str(int(ui_comp.get('font_size'))))\n",
    "        if style_attrs:\n",
    "            feat_str = feat_str + ' style=\"' + ';'.join(style_attrs) + '\"'\n",
    "        feat_str += '>'\n",
    "        if ui_comp.get('componentLabel') == 'Icon':\n",
    "            icon_text = ' '.join(ui_comp.get('iconClass').split('_')).strip()\n",
    "            feat_str += icon_text\n",
    "        elif ui_comp.get('componentLabel') == 'Text Button':\n",
    "            if ui_comp.get('buttonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('buttonClass').split('_')).strip()\n",
    "                feat_str += button_text\n",
    "            elif ui_comp.get('textButtonClass'):\n",
    "                button_text = ' '.join(ui_comp.get('textButtonClass').split('_')).strip()\n",
    "                feat_str += button_text\n",
    "            else:\n",
    "                feat_str += uic_text\n",
    "        else:\n",
    "            if ui_comp.get('text'):\n",
    "                feat_str += uic_text\n",
    "        feat_str += html_comp[1]\n",
    "        features.append((ui_comp.get('id'), feat_str))\n",
    "    if to_lower:\n",
    "        feat_str = feat_str.lower()\n",
    "    return {feat[0]: (feat[1], feat[2]) for feat in features}\n",
    "\n",
    "def features_to_str(gui, feat_method, n, m, to_lower, quote, style, id):\n",
    "    if feat_method == FEAT_METHOD_TEXT_ONLY:\n",
    "        return feat_method_text_only(gui, n, m, to_lower, quote, style)\n",
    "    elif feat_method == FEAT_METHOD_TEXT_COMP_TYPE:\n",
    "        return feat_method_text_comp_type(gui, n, m, to_lower, quote, style)\n",
    "    elif feat_method == FEAT_METHOD_TEXT_COMP_TYPE_RES_ID:\n",
    "        return feat_method_text_comp_type_res_id(gui, n, m, to_lower, quote, style, id)\n",
    "    elif feat_method == FEAT_METHOD_HTML:\n",
    "        return feat_method_html(gui, n, m, to_lower, quote, style)\n",
    "\n",
    "def filter_uic_groups(uic_groups):\n",
    "    filtered_uic_groups = []\n",
    "    # Sort ui comp group based on number of ui comps\n",
    "    uic_groups_sorted = sorted(uic_groups, key=lambda x: len(x['ui_comp_ids']), reverse=True)\n",
    "    for i, uic_group_1 in enumerate(uic_groups_sorted, 0):\n",
    "        subset_count = 0\n",
    "        for uic_group_2 in uic_groups_sorted[(i + 1):]:\n",
    "            if uic_group_1['id'] != uic_group_2['id']:\n",
    "                if set(uic_group_2['ui_comp_ids']).issubset(uic_group_1['ui_comp_ids']):\n",
    "                    subset_count += 1\n",
    "        if subset_count == 0:\n",
    "            filtered_uic_groups.append(uic_group_1)\n",
    "    return filtered_uic_groups\n",
    "\n",
    "def comp_in_uic(ui_comp_id, ui_comp_groups):\n",
    "    for uic in ui_comp_groups:\n",
    "        if ui_comp_id in uic.get('ui_comp_ids'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def structure_to_str(gui, feat_mappings, struct_method, style):\n",
    "    gui_cpy = copy.deepcopy(gui)\n",
    "    gui_mapping = {elem['id']: elem for elem in gui['ui_comps']}\n",
    "    if struct_method == STRUCT_METHOD_SIMPLE_BULLETS:\n",
    "        return '\\n- ' + '\\n- '.join([elem[0] for elem in feat_mappings.values()])\n",
    "    elif struct_method == STRUCT_METHOD_SIMPLE_BULLETS_SORTED:\n",
    "        uic_bounds = [(key, val, gui_mapping[key]['bounds']) for key, val in feat_mappings.items()]\n",
    "        uic_sorted = sorted(uic_bounds, key=lambda x: (x[2][1], x[2][0]))\n",
    "        return '\\n- ' + '\\n- '.join([elem[1][0] for elem in uic_sorted])\n",
    "    elif struct_method == STRUCT_METHOD_TWO_LEVEL_BULLETS or struct_method == STRUCT_METHOD_TWO_LEVEL_HTML:\n",
    "        uic_groups = gui_cpy['ui_comp_groups']\n",
    "        uic_groups = gui_cpy['ui_comp_groups']\n",
    "        single_ui_comps = [(ui_comp_id, vals[1]) for ui_comp_id, vals in feat_mappings.items()\n",
    "                           if not comp_in_uic(ui_comp_id, uic_groups)]\n",
    "        for elem in single_ui_comps:\n",
    "            uic_groups.append({\n",
    "                \"componentLabel\": \"Layout\",\n",
    "                \"bounds\": elem[1],\n",
    "                \"class\": \"android.widget.LinearLayout\",\n",
    "                \"bg_color\": \"#FFFFFF\",\n",
    "                \"ui_comp_ids\": [elem[0]],\n",
    "                'id': str(uuid.uuid4())\n",
    "            })\n",
    "        filtered_uic_groups = filter_uic_groups(uic_groups)\n",
    "        filtered_ui_comp_ids = []\n",
    "        for fuic in filtered_uic_groups:\n",
    "            filtered_ui_comp_ids.extend(fuic['ui_comp_ids'])\n",
    "        filtered_ui_comp_ids = set(filtered_ui_comp_ids)\n",
    "        all_ui_comp_ids = set([elem['id'] for elem in gui_cpy['ui_comps']])\n",
    "        missing_ui_comp_ids = all_ui_comp_ids.difference(filtered_ui_comp_ids)\n",
    "        uic_groups_sorted_len = sorted(uic_groups, key=lambda x: len(x['ui_comp_ids']), reverse=False)\n",
    "        matched_ui_comp_groups = {}\n",
    "        for miss_ui_comp_id in missing_ui_comp_ids:\n",
    "            for uic_group_len in uic_groups_sorted_len:\n",
    "                if miss_ui_comp_id in uic_group_len['ui_comp_ids']:\n",
    "                    if uic_group_len['id'] in matched_ui_comp_groups:\n",
    "                        matched_ui_comp_groups[uic_group_len['id']]['ui_comp_ids'].append(miss_ui_comp_id)\n",
    "                        break\n",
    "                    else:\n",
    "                        matched_ui_comp_groups[uic_group_len['id']] = {\n",
    "                            \"componentLabel\": \"Layout\",\n",
    "                            \"bounds\": uic_group_len['bounds'],\n",
    "                            \"class\": \"android.widget.LinearLayout\",\n",
    "                            \"bg_color\": \"#FFFFFF\",\n",
    "                            \"ui_comp_ids\": [miss_ui_comp_id],\n",
    "                            'id': str(uuid.uuid4())\n",
    "                        }\n",
    "                        break\n",
    "        filtered_uic_groups.extend([val for key, val in matched_ui_comp_groups.items()])\n",
    "        uic_groups_sorted = sorted(filtered_uic_groups, key=lambda x: (x['bounds'][1], x['bounds'][0]))\n",
    "        feat_str = ''\n",
    "        for uic_group in uic_groups_sorted:\n",
    "            uic_group['ui_comp_ids'] = [(feat_mappings.get(idd)[0], gui_mapping.get(idd)) for idd in\n",
    "                                        uic_group['ui_comp_ids']]\n",
    "            uic_group['ui_comp_ids'] = sorted(uic_group['ui_comp_ids'],\n",
    "                                              key=lambda x: (x[1]['bounds'][1], x[1]['bounds'][0]))\n",
    "            if struct_method == STRUCT_METHOD_TWO_LEVEL_BULLETS:\n",
    "                feat_str += '- ' + uic_group.get('componentLabel')\n",
    "                feat_str += '\\n\\t- ' + '\\n\\t- '.join([elem[0] for elem in uic_group['ui_comp_ids']]) + '\\n'\n",
    "            elif struct_method == STRUCT_METHOD_TWO_LEVEL_HTML:\n",
    "                html_mapping = html_comp_group_mapping.get(uic_group.get('componentLabel'))\n",
    "                feat_str += html_mapping[0]\n",
    "                style_attrs = []\n",
    "                feat_str += '>'\n",
    "                feat_str += '\\n\\t' + '\\n\\t'.join([elem[0] for elem in uic_group['ui_comp_ids']]) + '\\n'\n",
    "                feat_str += html_mapping[1] + '\\n'\n",
    "        return feat_str\n",
    "\n",
    "def get_str_repr_gui(gui, n, m, to_lower, quote, style, id, feat_method, struct_method):\n",
    "    feat_mappings = features_to_str(gui, feat_method, n, m, to_lower, quote, style, id)\n",
    "    final_str = structure_to_str(gui, feat_mappings, struct_method, style)\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "organization = \"organization id\"\n",
    "api_key = \"api key\"\n",
    "client = OpenAI(api_key=api_key, organization=organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt, model='gpt-4o', temp=0.7, n=1, max_tokens=15500, logprobs=True, top_logprobs=5, return_obj=True):\n",
    "    if logprobs:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "              temperature=temp,\n",
    "              n=n,\n",
    "              logprobs=logprobs,\n",
    "              top_logprobs=top_logprobs\n",
    "        )\n",
    "    else:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "              temperature=temp,\n",
    "              n=n,\n",
    "        )\n",
    "    return chat_completion if return_obj else [choice.message.content for choice in chat_completion.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion_multiple(prompts, model='gpt-4o', temp=0.7, n=1, max_tokens=15500, logprobs=True, top_logprobs=5, return_obj=True):\n",
    "    messages = []\n",
    "    for prompt in prompts:\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    if logprobs:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=messages,\n",
    "              temperature=temp,\n",
    "              n=n,\n",
    "              logprobs=logprobs,\n",
    "              top_logprobs=top_logprobs\n",
    "        )\n",
    "    else:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=messages,\n",
    "              temperature=temp,\n",
    "              n=n,\n",
    "        )\n",
    "    return chat_completion if return_obj else [choice.message.content for choice in chat_completion.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def respond_to_image(prompt, image, model=\"gpt-4o\", temp=0.7, n=1, max_tokens=15500, return_obj=True, timeout=300):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \n",
    "                [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{image}\",\n",
    "                            },\n",
    "                        },\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": temp,\n",
    "        \"n\": n,\n",
    "    }\n",
    "    chat_completion = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload, timeout=timeout).json()    \n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_image_multiple(prompts_and_images, model=\"gpt-4o\", temp=0.7, n=1, max_tokens=15500, return_obj=True, timeout=300):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    messages = []\n",
    "    for prompt, image in prompts_and_images:\n",
    "        if image is not None:\n",
    "            message = {\"role\": \"user\", \"content\": \n",
    "                        [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                                {\n",
    "                                    \"type\": \"image_url\",\n",
    "                                    \"image_url\": {\n",
    "                                    \"url\":  f\"data:image/jpeg;base64,{image}\",\n",
    "                                },\n",
    "                                }\n",
    "                        ]\n",
    "                      }\n",
    "        else:\n",
    "            message = {\"role\": \"user\", \"content\": \n",
    "                        [{\"type\": \"text\", \"text\": prompt}]\n",
    "                      }\n",
    "        messages.append(message)\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temp,\n",
    "        \"n\": n,\n",
    "    }\n",
    "    chat_completion = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload, timeout=timeout).json()\n",
    "    \n",
    "    return chat_completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Annotation and Reranking Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "\"\"\"\n",
    "Possible methods: \n",
    " - 'one_step': Use the raw GUI2String representation\n",
    " - 'two_step': Create a natural-language description based on the GUI2String representation\n",
    " - 'image': Use a screenshot of the GUI\n",
    "Possible ranking types:\n",
    " - 'binary': Assign the UI a relevance score of 1 or 0\n",
    " - 'three_level' and 'three_level_v2': Assign the UI a relevance score of 2, 1, or 0\n",
    " - 'one_to_ten': Assign the UI a relevance score between 1 and 10\n",
    " - 'one_to_ten_rate': Assign the UI a relevance score between 1 and 10 and give its feature set, design, and layout a score between 1 and 10\n",
    "\"\"\"\n",
    "\n",
    "def rank_and_filter(top_k, query, retrieve, method=\"two_step\", reasoning=False, ranking_type=\"binary\"):\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    for retrieved in top_k:\n",
    "        print(\"   \", len(ret))\n",
    "        try:\n",
    "            rico_id = retrieved[0]\n",
    "            conf = retrieved[1]\n",
    "        except:\n",
    "            rico_id = retrieved\n",
    "        resp = {\"id\": rico_id, \"score\": \"\", \"reasoning\": \"\", \"description\": \"\", \"prompt\": \"\", \"response\": \"\", \"sub_response\": \"\", \"full_response\": \"\", \"error\": 0}\n",
    "        gui = all_guis_with_comps[all_guis_with_comps['id'] == rico_id]['data'].values.tolist()[0]\n",
    "        desc = get_str_repr_gui(gui, n=30, m=30, to_lower=False, quote=True, style={}, id=True,\n",
    "                            feat_method=FEAT_METHOD_TEXT_COMP_TYPE_RES_ID,\n",
    "                            struct_method=STRUCT_METHOD_TWO_LEVEL_HTML)\n",
    "        \n",
    "        if method == \"two_step\":\n",
    "            prompt = \"I have extracted some information about the contents of a mobile UI from its source code:\\n\" + desc +\\\n",
    "            \"\\nDescribe its semantic contents succinctly.\"\n",
    "            sub_resp = generate_completion(prompt, temp=0.7)\n",
    "            desc_gpt = sub_resp.choices[0].message.content\n",
    "            resp[\"description\"] = desc_gpt\n",
    "            resp[\"sub_response\"] = sub_resp\n",
    "\n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \\\"\" + query + \"\\\"\\n My retrieval algorithm returned a UI \" +\\\n",
    "            \"that can be described as follows:\\n\\\"\" + desc_gpt +\\\n",
    "            \"\\\"\\nDoes this specific UI fit your query? Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "        elif method == \"image\":\n",
    "            image = encode_image(rico_path + str(rico_id) + \".jpg\")\n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \\\"\" + query + \"\\\"\\n My retrieval algorithm returned \" +\\\n",
    "            \"the following image.\" +\\\n",
    "            \"\\nDoes this specific UI fit your query? Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "        else: \n",
    "            #one_step\n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \" + query + \"\\n My retrieval algorithm returned a UI \" +\\\n",
    "            \"containing the following elements:\\n\" + desc + \"Does the UI fit your query? \" +\\\n",
    "            \"Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "            resp[\"description\"] = desc\n",
    "\n",
    "        if ranking_type == \"binary\":\n",
    "            prompt += \"Answer in json format, like so: {'relevance_score': X}, X being 1 if the UI is relevant and 0 if it is irrelevant.\"\n",
    "        elif ranking_type == \"one_to_ten\":\n",
    "            prompt += \"Assign the UI a relevance score between 1 and 10, with 1 meaning that it is not relevant at all, and 10 meaning that it \" +\\\n",
    "                      \"is completely relevant, and the numbers inbetween indicating various levels of relevance depending on features and functions \" +\\\n",
    "                      \"present in the UI. Answer in json format, like so: {'relevance_score': X}. \"\n",
    "        elif ranking_type == \"one_to_ten_rate\":\n",
    "            prompt += \"Assign the UI a relevance score between 1 and 10, with 1 meaning that it is not relevant at all, and 10 meaning that it \" +\\\n",
    "                      \"is completely relevant, and the numbers inbetween indicating various levels of relevance depending on features and functions \" +\\\n",
    "                      \"present in the UI. Also rate the UI in the same way regarding these attributes: feature set (related to the query), \" +\\\n",
    "                      \"design, and layout. Answer in json format, like so: {'relevance_score': X, 'feature_set': X, 'design': X, 'layout': X}. \"\n",
    "            resp[\"feature_set\"] = \"\"\n",
    "            resp[\"design\"] = \"\"\n",
    "            resp[\"layout\"] = \"\"\n",
    "        elif ranking_type == \"three_level\":\n",
    "            prompt += \"Assign the UI a relevance score of 0, 1, or 2. You should give it a 2 if both the function and the content domain of the UI align with \" +\\\n",
    "            \"the query, a 1 if only the function fits and a 0 if neither or only the content domain fits. \" +\\\n",
    "            \"Answer in json format, like so: {'relevance_score': X}, with X being the relevance score you assign to the UI. \"\n",
    "        else:\n",
    "            #three_level_v2\n",
    "            prompt += \"Assign the UI a relevance score of 0, 1, or 2. You should give it a 2 if it is relevant to the query, a 0 if it is irrelevant to the query \" +\\\n",
    "            \"and a 1 if it is somewhere inbetween. Answer in json format, like so: {'relevance_score': X}, with X being the relevance score you assign to the UI. \"\n",
    "        \n",
    "        if reasoning:\n",
    "            prompt += \"Briefly explain your chain of thoughts and append it to the json like so: {'relevance_score': X, 'reasoning': 'your reasoning'}. \"\n",
    "        \n",
    "\n",
    "        resp[\"prompt\"] = prompt\n",
    "\n",
    "        try:\n",
    "            if method == \"image\":\n",
    "                full_response = respond_to_image(prompt, image, temp=0.7)\n",
    "                response = full_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            else:\n",
    "                full_response = generate_completion(prompt, temp=0.7)\n",
    "                response = full_response.choices[0].message.content\n",
    "    \n",
    "            resp[\"response\"] = response\n",
    "            resp[\"full_response\"] = full_response\n",
    "    \n",
    "            json_start_loc = response.find(\"{\")\n",
    "            json_end_loc = response.rfind(\"}\")+1\n",
    "            json_str = response[json_start_loc:json_end_loc]\n",
    "            try:\n",
    "                response_json = literal_eval(json_str)\n",
    "                resp[\"score\"] = response_json[\"relevance_score\"]\n",
    "                resp[\"reasoning\"] = response_json[\"reasoning\"]\n",
    "                if ranking_type == \"one_to_ten_rate\":\n",
    "                        resp[\"feature_set\"] = response_json[\"feature_set\"]\n",
    "                        resp[\"design\"] = response_json[\"design\"]\n",
    "                        resp[\"layout\"] = response_json[\"layout\"]\n",
    "            except Exception as e:\n",
    "                print(type(e), e)\n",
    "                resp[\"error\"] = 1\n",
    "                resp[\"score\"] = 0\n",
    "                resp[\"reasoning\"] = \"\"\n",
    "                if ranking_type == \"one_to_ten_rate\":\n",
    "                        resp[\"feature_set\"] = 0\n",
    "                        resp[\"design\"] = 0\n",
    "                        resp[\"layout\"] = 0\n",
    "        except Exception as e:\n",
    "            print(type(e), e)\n",
    "            resp[\"error\"] = 2\n",
    "            resp[\"score\"] = 0\n",
    "            resp[\"reasoning\"] = \"\"\n",
    "            if ranking_type == \"one_to_ten_rate\":\n",
    "                    resp[\"feature_set\"] = 0\n",
    "                    resp[\"design\"] = 0\n",
    "                    resp[\"layout\"] = 0\n",
    "\n",
    "        ret.append(resp)\n",
    "            \n",
    "    return ret[:retrieve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Same approach as above, but the n parameter lets you set the number of answers you want, also computes averages and standard deviations for ratings\n",
    "\"\"\"\n",
    "\n",
    "def rank_and_filter_n(top_k, query, retrieve, method=\"two_step\", reasoning=False, ranking_type=\"binary\", n=1):\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    for retrieved in top_k:\n",
    "        print(\"   \", len(ret))\n",
    "        try:\n",
    "            rico_id = retrieved[0]\n",
    "            conf = retrieved[1]\n",
    "        except:\n",
    "            rico_id = retrieved\n",
    "        resp = {\"id\": rico_id, \"scores\": [], \"reasonings\": [], \"description\": \"\", \"prompt\": \"\", \"responses\": [], \n",
    "                \"sub_response\": \"\", \"full_responses\": [], \"errors\": [], \"avg_score\": \"\", \"std_dev\": \"\"}\n",
    "        gui = all_guis_with_comps[all_guis_with_comps['id'] == rico_id]['data'].values.tolist()[0]\n",
    "        desc = get_str_repr_gui(gui, n=30, m=30, to_lower=False, quote=True, style={}, id=True,\n",
    "                            feat_method=FEAT_METHOD_TEXT_COMP_TYPE_RES_ID,\n",
    "                            struct_method=STRUCT_METHOD_TWO_LEVEL_HTML)\n",
    "        \n",
    "        if method == \"two_step\":\n",
    "            prompt = \"I have extracted some information about the contents of a mobile UI from its source code:\\n\" + desc +\\\n",
    "            \"\\nDescribe its semantic contents succinctly.\"\n",
    "            sub_resp = generate_completion(prompt, temp=0.7)\n",
    "            desc_gpt = sub_resp.choices[0].message.content\n",
    "            resp[\"description\"] = desc_gpt\n",
    "            resp[\"sub_response\"] = sub_resp\n",
    "\n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \\\"\" + query + \"\\\"\\n My retrieval algorithm returned a UI \" +\\\n",
    "            \"that can be described as follows:\\n\\\"\" + desc_gpt +\\\n",
    "            \"\\\"\\nDoes this specific UI fit your query? Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "        elif method == \"image\":\n",
    "            image = encode_image(rico_path + str(rico_id) + \".jpg\")\n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \\\"\" + query + \"\\\"\\n My retrieval algorithm returned \" +\\\n",
    "            \"the following image.\" +\\\n",
    "            \"\\nDoes this specific UI fit your query? Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "        else: \n",
    "            prompt = \"You are a user searching for mobile UIs to base your design\" +\\\n",
    "            \" on using the following query: \" + query + \"\\n My retrieval algorithm returned a UI \" +\\\n",
    "            \"containing the following elements:\\n\" + desc + \"Does the UI fit your query? \" +\\\n",
    "            \"Do not think about what the rest of the app might contain and do not be afraid to say no. \" +\\\n",
    "            \"I would rather filter out a (semi-) relevant UI than not remove an irrelevant UI.\"\n",
    "            resp[\"description\"] = desc\n",
    "\n",
    "        if ranking_type == \"binary\":\n",
    "            prompt += \"Answer in json format, like so: {'relevance_score': X}, X being 1 if the UI is relevant and 0 if it is irrelevant.\"\n",
    "        elif ranking_type == \"one_to_ten\":\n",
    "            prompt += \"Assign the UI a relevance score between 1 and 10, with 1 meaning that it is not relevant at all, and 10 meaning that it \" +\\\n",
    "                      \"is completely relevant, and the numbers inbetween indicating various levels of relevance depending on features and functions \" +\\\n",
    "                      \"present in the UI. Answer in json format, like so: {'relevance_score': X}. \"\n",
    "        elif ranking_type == \"one_to_ten_rate\":\n",
    "            prompt += \"Assign the UI a relevance score between 1 and 10, with 1 meaning that it is not relevant at all, and 10 meaning that it \" +\\\n",
    "                      \"is completely relevant, and the numbers inbetween indicating various levels of relevance depending on features and functions \" +\\\n",
    "                      \"present in the UI. Also rate the UI in the same way regarding these attributes: feature set (related to the query), \" +\\\n",
    "                      \"design, and layout. Answer in json format, like so: {'relevance_score': X, 'feature_set': X, 'design': X, 'layout': X}. \"\n",
    "            resp[\"feature_set\"] = []\n",
    "            resp[\"design\"] = []\n",
    "            resp[\"layout\"] = []\n",
    "        elif ranking_type == \"three_level\":\n",
    "            prompt += \"Assign the UI a relevance score of 0, 1, or 2. You should give it a 2 if both the function and the content domain of the UI align with \" +\\\n",
    "            \"the query, a 1 if only the function fits and a 0 if neither or only the content domain fits. \" +\\\n",
    "            \"Answer in json format, like so: {'relevance_score': X}, with X being the relevance score you assign to the UI. \"\n",
    "        else:\n",
    "            prompt += \"Assign the UI a relevance score of 0, 1, or 2. You should give it a 2 if it is relevant to the query, a 0 if it is irrelevant to the query \" +\\\n",
    "            \"and a 1 if it is somewhere inbetween. Answer in json format, like so: {'relevance_score': X}, with X being the relevance score you assign to the UI. \"\n",
    "        \n",
    "        if reasoning:\n",
    "            prompt += \"Briefly explain your chain of thoughts and append it to the json like so: {'relevance_score': X, 'reasoning': 'your reasoning'}. \"\n",
    "        \n",
    "\n",
    "        resp[\"prompt\"] = prompt\n",
    "        responses = []\n",
    "        \n",
    "        try:\n",
    "            if method == \"image\":\n",
    "                full_response = respond_to_image(prompt, image, temp=0.7, n=n)\n",
    "                for i in range(n):\n",
    "                    responses.append(full_response[\"choices\"][i][\"message\"][\"content\"])\n",
    "            else:\n",
    "                full_response = generate_completion(prompt, temp=0.7, n=n)\n",
    "                for i in range(n):\n",
    "                    responses.append(full_response.choices[i].message.content)\n",
    "            \n",
    "            resp[\"responses\"] = responses\n",
    "            resp[\"full_response\"] = full_response\n",
    "            \n",
    "            for response in responses:\n",
    "                json_start_loc = response.find(\"{\")\n",
    "                json_end_loc = response.rfind(\"}\")+1\n",
    "                json_str = response[json_start_loc:json_end_loc]\n",
    "                try:\n",
    "                    response_json = literal_eval(json_str)\n",
    "                    resp[\"scores\"].append(response_json[\"relevance_score\"])\n",
    "                    resp[\"reasonings\"].append(response_json[\"reasoning\"])\n",
    "                    resp[\"errors\"].append(0)\n",
    "                    if ranking_type == \"one_to_ten_rate\":\n",
    "                        resp[\"feature_set\"].append(response_json[\"feature_set\"])\n",
    "                        resp[\"design\"].append(response_json[\"design\"])\n",
    "                        resp[\"layout\"].append(response_json[\"layout\"])\n",
    "                except Exception as e:\n",
    "                    resp[\"scores\"].append(0)\n",
    "                    resp[\"reasonings\"].append(\"\")\n",
    "                    print(type(e), e)\n",
    "                    resp[\"errors\"].append(1)\n",
    "                    if ranking_type == \"one_to_ten_rate\":\n",
    "                        resp[\"feature_set\"].append(0)\n",
    "                        resp[\"design\"].append(0)\n",
    "                        resp[\"layout\"].append(0)\n",
    "            avg_score = np.mean(resp[\"scores\"])\n",
    "            std_dev = np.std(resp[\"scores\"])\n",
    "            resp[\"avg_score\"] = avg_score\n",
    "            resp[\"std_dev\"] = std_dev\n",
    "            \n",
    "        except Exception as e:\n",
    "            resp[\"scores\"].append(0)\n",
    "            resp[\"reasonings\"].append(\"\")\n",
    "            print(type(e), e)\n",
    "            resp[\"errors\"].append(2)\n",
    "            resp[\"avg_score\"] = 0\n",
    "            resp[\"std_dev\"] = 0\n",
    "\n",
    "        ret.append(resp)\n",
    "            \n",
    "    return ret[:retrieve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Possible methods:\n",
    " - 'one_step': Use the raw GUI2String representation\n",
    " - 'two_step': Create a natural-language description based on the GUI2String representation\n",
    " - 'image': Use a screenshot of the GUI\n",
    "Possible ranking types:\n",
    " - 'v1': Reorder the list of UIs based on their relevance to the query\n",
    " - 'v2': Reorder the list of UIs based on their relevance to the query, their design, their layout, and their usability\n",
    "\"\"\"\n",
    "\n",
    "def rank_and_rerank(top_k, query, retrieve, method=\"two_step\", reasoning=False, ranking_type=\"v1\", temp=0.7):\n",
    "\n",
    "    descs = []\n",
    "    sub_responses = []\n",
    "    ret = {\"retrieved\": top_k, \"ranked\": \"\", \"ranked_unmapped\": \"\", \"reasoning\": \"\", \"descriptions\": \"\", \"prompt\": \"\", \"response\": \"\", \"full_response\": \"\", \"sub_responses\": [], \"full_prompts\": \"\", \"id_mappings\": \"\", \"error\": 0}\n",
    "    id_mappings = {}\n",
    "    counter = 0\n",
    "    \n",
    "    for retrieved in top_k:\n",
    "        try:\n",
    "            rico_id = retrieved[0]\n",
    "            conf = retrieved[1]\n",
    "        except:\n",
    "            rico_id = retrieved\n",
    "        gui = all_guis_with_comps[all_guis_with_comps['id'] == rico_id]['data'].values.tolist()[0]\n",
    "        desc = get_str_repr_gui(gui, n=30, m=30, to_lower=False, quote=True, style={}, id=True,\n",
    "                            feat_method=FEAT_METHOD_TEXT_COMP_TYPE_RES_ID,\n",
    "                            struct_method=STRUCT_METHOD_TWO_LEVEL_HTML)\n",
    "        id_mappings[counter] = rico_id\n",
    "        \n",
    "        if method == \"two_step\":\n",
    "            prompt = \"I have extracted some information about the contents of a mobile UI from its source code:\\n\" + desc +\\\n",
    "            \"\\nDescribe its semantic contents succinctly.\"\n",
    "            sub_resp = generate_completion(prompt, temp=temp)\n",
    "            desc_gpt = sub_resp.choices[0].message.content\n",
    "            descs.append(\"id: \" + str(counter) + \", description:\" + desc_gpt)\n",
    "            sub_responses.append(sub_resp)\n",
    "        elif method == \"image\":\n",
    "            image = encode_image(rico_path + str(rico_id) + \".jpg\")\n",
    "            descs.append((\"id: \" + str(counter), image))\n",
    "        else:\n",
    "            descs.append(\"id: \" + str(counter) + \", description:\" + desc)\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    if method != \"image\":\n",
    "        ret[\"descriptions\"] = descs\n",
    "        ret[\"sub_responses\"] = sub_responses\n",
    "\n",
    "    ret[\"id_mappings\"] = id_mappings\n",
    "\n",
    "    if ranking_type == \"v1\":\n",
    "        prompt = \"You are a user searching for mobile UIs to base your design on using the following query: \\\"\" + query + \"\\\"\\nMy retrieval algorithm returned\" +\\\n",
    "        \" some UIs, whose ids and descriptions I will send you next. Please order the UIs based on how relevant you think they are to the query and answer\" +\\\n",
    "        \" with a list of ids (beginning with the most relevant UI). Answer in json format like so (this ranking is just a random example: \" +\\\n",
    "        \"{'ranking': 0, 1, 8, 9, 18, 19, 7, 5, 3, 6, 2, 15, 4, 10, 13, 11, 12, 17, 16, 14}\" \n",
    "              \n",
    "        if reasoning: \n",
    "            prompt += \" Additionally, shortly explain your chain of thoughts for the ranking, going into detail about each UI and append it to \" \\\n",
    "                      \"the json like so (this ranking is just a random example): {'ranking': 0, 1, 8, 9, 18, 19, 7, 5, 3, 6, 2, 15, 4, 10, 13, 11, 12, 17, 16, 14,\" +\\\n",
    "                      \"'reasoning': '0: reasoning, 1: reasoning, 8: reasoning, 9: reasoning, 18: reasoning, 19: reasoning, 7: reasoning, 5: reasoning, \" +\\\n",
    "                      \"3: reasoning, 6: reasoning, 2: reasoning, 15: reasoning, 4: reasoning, 10: reasoning, 13: reasoning, 11: reasoning, 12: reasoning, \" + \\\n",
    "                      \"17: reasoning, 16: reasoning, 14: reasoning'}. Make sure your response can be correctly parsed\" \\\n",
    "                      \" as json data. Also, be sure that the lists in the ranking and the reasoning sections are the same and that you don't miss any of the UIs in the list.\"\n",
    "    else:\n",
    "        prompt = \"You are a user searching for mobile UIs to base your design on using the following query: \\\"\" + query + \"\\\"\\nMy retrieval algorithm returned\" +\\\n",
    "        \" some UIs, whose ids and descriptions I will send you next. Please judge the UIs based on their design, their layout, their usability, and how relevant you think their feature set is to the query,\" +\\\n",
    "        \" and answer with a list of ids (beginning with the best and most relevant UI). Answer in json format like so (this ranking is just a random example: \" +\\\n",
    "        \"{'ranking': 0, 1, 8, 9, 18, 19, 7, 5, 3, 6, 2, 15, 4, 10, 13, 11, 12, 17, 16, 14}\" \n",
    "              \n",
    "        if reasoning: \n",
    "            prompt += \" Additionally, shortly explain your chain of thoughts for the ranking, going into detail about each UI and append it to \" \\\n",
    "                      \"the json like so (this ranking is just a random example): {'ranking': 0, 1, 8, 9, 18, 19, 7, 5, 3, 6, 2, 15, 4, 10, 13, 11, 12, 17, 16, 14,\" +\\\n",
    "                      \"'reasoning': '0: reasoning, 1: reasoning, 8: reasoning, 9: reasoning, 18: reasoning, 19: reasoning, 7: reasoning, 5: reasoning, \" +\\\n",
    "                      \"3: reasoning, 6: reasoning, 2: reasoning, 15: reasoning, 4: reasoning, 10: reasoning, 13: reasoning, 11: reasoning, 12: reasoning, \" + \\\n",
    "                      \"17: reasoning, 16: reasoning, 14: reasoning'}. Make sure your response can be correctly parsed\" \\\n",
    "                      \" as json data. Also, be sure that the lists in the ranking and the reasoning sections are the same and that you don't miss any of the UIs in the list.\"\n",
    "\n",
    "    ret[\"prompt\"] = prompt\n",
    "\n",
    "    if method != \"image\":\n",
    "        prompts = [prompt]\n",
    "    else:\n",
    "        prompts = [(prompt, None)]\n",
    "        \n",
    "    prompts.extend(descs)\n",
    "    ret[\"full_prompts\"] = prompts\n",
    "\n",
    "    try:\n",
    "        if method != \"image\":\n",
    "            full_response = generate_completion_multiple(prompts, temp=temp, timeout=420)\n",
    "            response = full_response.choices[0].message.content\n",
    "        else:\n",
    "            full_response = respond_to_image_multiple(prompts, temp=temp, timeout=420)\n",
    "            response = full_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "        json_start_loc = response.find(\"{\")\n",
    "        json_end_loc = response.rfind(\"}\")+1\n",
    "        json_str = response[json_start_loc:json_end_loc]\n",
    "        try:\n",
    "            response_json = literal_eval(json_str)\n",
    "            ranked_unmapped = response_json[\"ranking\"]\n",
    "            reasoning = response_json[\"reasoning\"]\n",
    "            ranked = []\n",
    "            for mapped_id in ranked_unmapped:\n",
    "                ranked.append(id_mappings[int(mapped_id)])\n",
    "            ret[\"ranked_unmapped\"] = ranked_unmapped\n",
    "            ret[\"ranked\"] = ranked\n",
    "            ret[\"reasoning\"] = reasoning\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(type(e), e)\n",
    "            print(json_str)\n",
    "            print(\"HERE\")\n",
    "            ret[\"error\"] = 1\n",
    "        \n",
    "        ret[\"response\"] = response\n",
    "        \n",
    "        ret[\"full_response\"] = full_response\n",
    "    except Exception as e:\n",
    "        print(type(e), e)\n",
    "        ret[\"error\"] = 2\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Possible methods:\n",
    " - 'one_step': Use the raw GUI2String representation\n",
    " - 'two_step': Create a natural-language description based on the GUI2String representation\n",
    " - 'image': Use a screenshot of the GUI\n",
    "\n",
    "Returns a single UI that is deemed to be the most relevant to the query\n",
    "\"\"\"\n",
    "\n",
    "def return_best(top_k, query, method=\"two_step\", reasoning=False):\n",
    "    descs = []\n",
    "    sub_responses = []\n",
    "    ret = {\"retrieved\": top_k, \"best\": \"\", \"descriptions\": \"\", \"prompt\": \"\", \"response\": \"\", \"full_response\": \"\", \"sub_responses\": [], \"full_prompts\": \"\"}\n",
    "\n",
    "    for retrieved in top_k:\n",
    "        try:\n",
    "            rico_id = retrieved[0]\n",
    "            conf = retrieved[1]\n",
    "        except:\n",
    "            rico_id = retrieved\n",
    "        print(rico_id)\n",
    "        gui = all_guis_with_comps[all_guis_with_comps['id'] == rico_id]['data'].values.tolist()[0]\n",
    "        desc = get_str_repr_gui(gui, n=30, m=30, to_lower=False, quote=True, style={}, id=True,\n",
    "                            feat_method=FEAT_METHOD_TEXT_COMP_TYPE_RES_ID,\n",
    "                            struct_method=STRUCT_METHOD_TWO_LEVEL_HTML)\n",
    "        \n",
    "        if method == \"two_step\":\n",
    "            prompt = \"I have extracted some information about the contents of a mobile UI from its source code:\\n\" + desc +\\\n",
    "            \"\\nDescribe its semantic contents succinctly.\"\n",
    "            sub_resp = generate_completion(prompt, temp=0.7)\n",
    "            desc_gpt = sub_resp.choices[0].message.content\n",
    "            descs.append(\"id: \" + str(rico_id) + \", description:\" + desc_gpt)\n",
    "            sub_responses.append(sub_resp)\n",
    "        elif method == \"image\":\n",
    "            image = encode_image(rico_path + str(rico_id) + \".jpg\")\n",
    "            descs.append((\"id: \" + str(rico_id), image))\n",
    "        else:\n",
    "            descs.append(\"id: \" + str(rico_id) + \", description:\" + desc)\n",
    "    if method != \"image\":\n",
    "        ret[\"descriptions\"] = descs\n",
    "        ret[\"sub_responses\"] = sub_responses\n",
    "    \n",
    "    prompt = \"You are a user searching for mobile UIs to base your design on using the following query: \\\"\" + query + \"\\\"\\nMy retrieval algorithm returned\" +\\\n",
    "    \" some UIs, whose ids and descriptions I will send you next. Please rate the UIs in terms of the features they provide, usability, design, and how well\" +\\\n",
    "    \" they fit the query and return the id of the UI you think is the best overall at the end of your answer.\"\n",
    "          \n",
    "    if reasoning: \n",
    "        prompt += \" Additionally, shortly explain your chain of thoughts, going into detail about each UI.\"\n",
    "\n",
    "    ret[\"prompt\"] = prompt\n",
    "\n",
    "    if method != \"image\":\n",
    "        prompts = [prompt]\n",
    "    else:\n",
    "        prompts = [(prompt, None)]\n",
    "        \n",
    "    prompts.extend(descs)\n",
    "    #print(prompts)\n",
    "    ret[\"full_prompts\"] = prompts\n",
    "\n",
    "    if method != \"image\":\n",
    "        full_response = generate_completion_multiple(prompts,  temp=0.7)\n",
    "        response = full_response.choices[0].message.content\n",
    "    else:\n",
    "        full_response = respond_to_image_multiple(prompts,  temp=0.7)\n",
    "        response = full_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    best = re.findall(r'\\d+', response)[-1]\n",
    "    \n",
    "    ret[\"response\"] = response\n",
    "    ret[\"best\"] = best\n",
    "    ret[\"full_response\"] = full_response\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Examples/Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a shopping app presenting a t-shirt for purchase.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank and Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three Level Ranking\n",
    "\n",
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 1\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"two_step\", reasoning=True, ranking_type=\"three_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"].usage.prompt_tokens\n",
    "    completion_tokens = resp[\"full_response\"].usage.completion_tokens\n",
    "    sub_input_tokens = resp[\"sub_response\"].usage.prompt_tokens\n",
    "    sub_completion_tokens = resp[\"sub_response\"].usage.completion_tokens\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary ranking\n",
    "\n",
    "#query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"two_step\", reasoning=True, ranking_type=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"].usage.prompt_tokens\n",
    "    completion_tokens = resp[\"full_response\"].usage.completion_tokens\n",
    "    sub_input_tokens = resp[\"sub_response\"].usage.prompt_tokens\n",
    "    sub_completion_tokens = resp[\"sub_response\"].usage.completion_tokens\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three level ranking\n",
    "\n",
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 1\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"image\", reasoning=True, ranking_type=\"three_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"][\"usage\"][\"prompt_tokens\"]\n",
    "    completion_tokens = resp[\"full_response\"][\"usage\"][\"completion_tokens\"]\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary ranking\n",
    "\n",
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 1\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"image\", reasoning=True, ranking_type=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"][\"usage\"][\"prompt_tokens\"]\n",
    "    completion_tokens = resp[\"full_response\"][\"usage\"][\"completion_tokens\"]\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three level ranking\n",
    "\n",
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 1\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"one_step\", reasoning=True, ranking_type=\"three_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"].usage.prompt_tokens\n",
    "    completion_tokens = resp[\"full_response\"].usage.completion_tokens\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary ranking\n",
    "\n",
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 1\n",
    "top_k = rank(query)[:retrieve]\n",
    "filtered_top_k = rank_and_filter(top_k, query, retrieve, method=\"one_step\", reasoning=True, ranking_type=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for resp in filtered_top_k:\n",
    "    input_tokens = resp[\"full_response\"].usage.prompt_tokens\n",
    "    completion_tokens = resp[\"full_response\"].usage.completion_tokens\n",
    "    total_input_tokens += input_tokens\n",
    "    total_completion_tokens += completion_tokens\n",
    "    rico_id = resp[\"id\"]\n",
    "    img = Image.open(rico_path + str(rico_id) + \".jpg\")\n",
    "    img = img.resize((1080, 1920))\n",
    "    plt.imshow(img)\n",
    "    plt.grid(False)\n",
    "    title = str(rico_id) \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    if resp[\"description\"] != \"\":\n",
    "        print(resp[\"description\"])\n",
    "        print()\n",
    "    print(resp[\"response\"])\n",
    "    print(resp[\"reasoning\"])\n",
    "    print(resp[\"score\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", total_input_tokens)\n",
    "print(\"COMPLETION\", total_completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank and Rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = rank_and_rerank(top_k, query, retrieve, method=\"two_step\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for sub_resp in reranked[\"sub_responses\"]:\n",
    "    sub_input_tokens = sub_resp.usage.prompt_tokens\n",
    "    sub_completion_tokens = sub_resp.usage.completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"].usage.prompt_tokens)\n",
    "print(\"COMPLETION\", reranked[\"full_response\"].usage.completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)\n",
    "\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = rank_and_rerank(top_k, query, retrieve, method=\"one_step\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"].usage.prompt_tokens)\n",
    "print(\"COMPLETION\", reranked[\"full_response\"].usage.completion_tokens)\n",
    "print(\"RANKING\", reranked[\"ranked\"])\n",
    "\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = rank_and_rerank(top_k, query, retrieve, method=\"image\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"][\"usage\"][\"prompt_tokens\"])\n",
    "print(\"COMPLETION\", reranked[\"full_response\"][\"usage\"][\"completion_tokens\"])\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = return_best(top_k, query, method=\"two_step\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "print(reranked[\"best\"])\n",
    "\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for sub_resp in reranked[\"sub_responses\"]:\n",
    "    sub_input_tokens = sub_resp.usage.prompt_tokens\n",
    "    sub_completion_tokens = sub_resp.usage.completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"].usage.prompt_tokens)\n",
    "print(\"COMPLETION\", reranked[\"full_response\"].usage.completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)\n",
    "\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = return_best(top_k, query, method=\"one_step\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "print(reranked[\"best\"])\n",
    "\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for sub_resp in reranked[\"sub_responses\"]:\n",
    "    sub_input_tokens = sub_resp.usage.prompt_tokens\n",
    "    sub_completion_tokens = sub_resp.usage.completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"].usage.prompt_tokens)\n",
    "print(\"COMPLETION\", reranked[\"full_response\"].usage.completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)\n",
    "\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"A page from a music player app, presenting playback controls for audio content\"\n",
    "retrieve = 5\n",
    "top_k = rank(query)[:retrieve]\n",
    "reranked = return_best(top_k, query, method=\"image\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reranked[\"response\"])\n",
    "print(reranked[\"best\"])\n",
    "\n",
    "total_sub_input_tokens = 0\n",
    "total_sub_completion_tokens = 0\n",
    "for sub_resp in reranked[\"sub_responses\"]:\n",
    "    sub_input_tokens = sub_resp.usage.prompt_tokens\n",
    "    sub_completion_tokens = sub_resp.usage.completion_tokens\n",
    "    total_sub_input_tokens += sub_input_tokens\n",
    "    total_sub_completion_tokens += sub_completion_tokens\n",
    "\n",
    "print(\"\\nUSAGE\")\n",
    "print(\"INPUT\", reranked[\"full_response\"].usage.prompt_tokens)\n",
    "print(\"COMPLETION\", reranked[\"full_response\"].usage.completion_tokens)\n",
    "print(\"SUB INPUT\", total_sub_input_tokens)\n",
    "print(\"SUB COMPLETION\", total_sub_completion_tokens)\n",
    "\n",
    "show_images(reranked[\"retrieved\"], rico_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Annotation CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up (run one time only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(data_path + \"Description Dataset/dataset.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = dataset.sample(15)\n",
    "rand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate = []\n",
    "for index, row in rand.iterrows():\n",
    "    top_10 = rank(row[\"Summary\"])[:10]\n",
    "    for i in range(10):\n",
    "        anno_row = {\"query\": row[\"Summary\"], \"id\": top_10[i][0], \"annotation\": 0}\n",
    "        annotate.append(anno_row)\n",
    "\n",
    "anno_df = pd.DataFrame(annotate)\n",
    "anno_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_df.to_csv(data_path + \"annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate with GPT (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = pd.read_csv(data_path + \"annotations.csv\", sep = \";\")\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated[\"two_step\"] = 0\n",
    "annotated[\"one_step\"] = 0\n",
    "annotated[\"image\"] = 0\n",
    "annotated[\"two_step_reasoning\"] = \"\"\n",
    "annotated[\"one_step_reasoning\"] = \"\"\n",
    "annotated[\"image_reasoning\"] = \"\"\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = [\"two_step\", \"one_step\", \"image\"]\n",
    "count = 0\n",
    "for index, row in annotated.iterrows():\n",
    "    query = row[\"query\"]\n",
    "    rico_id = row[\"rico_id\"]\n",
    "    for method in methods:\n",
    "        ranked = rank_and_filter([(rico_id, 0.0)], query, 1, method=method, ranking_type=\"binary\", reasoning = True)\n",
    "        score = ranked[0][\"score\"]\n",
    "        reasoning = ranked[0][\"reasoning\"]\n",
    "        annotated.at[index, method] = score\n",
    "        annotated.at[index, method + \"_reasoning\"] = reasoning\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated.to_csv(data_path + \"annotations_with_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate with GPT (three-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = pd.read_csv(data_path + \"annotations_three_level.csv\", sep = \";\")\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated[\"two_step\"] = 0\n",
    "annotated[\"one_step\"] = 0\n",
    "annotated[\"image\"] = 0\n",
    "annotated[\"two_step_reasoning\"] = \"\"\n",
    "annotated[\"one_step_reasoning\"] = \"\"\n",
    "annotated[\"image_reasoning\"] = \"\"\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = [\"two_step\", \"one_step\", \"image\"]\n",
    "count = 0\n",
    "for index, row in annotated.iterrows():\n",
    "    print(index)\n",
    "    query = row[\"query\"]\n",
    "    rico_id = row[\"rico_id\"]\n",
    "    for method in methods:\n",
    "        ranked = rank_and_filter([(rico_id, 0.0)], query, 1, method=method, ranking_type=\"three_level\", reasoning = True)\n",
    "        score = ranked[0][\"score\"]\n",
    "        reasoning = ranked[0][\"reasoning\"]\n",
    "        annotated.at[index, method] = score\n",
    "        annotated.at[index, method + \"_reasoning\"] = reasoning\n",
    "annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated.to_csv(data_path + \"annotations_three_level_with_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Annotate Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldstandard_csv = pd.read_csv(data_path + \"goldstandard.csv\")\n",
    "goldstandard_csv[\"gui_indexes\"] = goldstandard_csv[\"gui_indexes\"].apply(literal_eval)\n",
    "goldstandard_csv[\"relevance\"] = goldstandard_csv[\"relevance\"].apply(literal_eval)\n",
    "goldstandard_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = goldstandard_csv.iloc[0]\n",
    "reranked = rank_and_rerank(test_data[\"gui_indexes\"][:3], test_data[\"query\"], retrieve=3, method=\"image\", reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"gui_indexes\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked[\"ranked\"] = list(map(int, reranked[\"ranked\"]))\n",
    "print(reranked[\"ranked\"])\n",
    "print(reranked[\"id_mappings\"])\n",
    "print(reranked[\"ranked_unmapped\"])\n",
    "print(reranked[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(test_data[\"gui_indexes\"]) == set(reranked[\"ranked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(reranked[\"full_response\"][\"usage\"][\"prompt_tokens\"]))\n",
    "print(reranked[\"full_response\"][\"usage\"][\"completion_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-level/Binary annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anno = goldstandard_csv.copy()\n",
    "\n",
    "methods = [\"image\"]\n",
    "ranking_type = \"one_to_ten\n",
    "for method in methods:\n",
    "    anno[method] = \"\"\n",
    "    anno[method + \"_reasonings\"] = \"\"\n",
    "    anno[method + \"_errors\"] = \"\"\n",
    "    anno[method + \"_responses\"] = \"\"\n",
    "    anno[method + \"_prompt_tokens\"] = \"\"\n",
    "    anno[method + \"_completion_tokens\"] = \"\"\n",
    "    anno[method + \"_total_prompt_tokens\"] = \"\"\n",
    "    anno[method + \"_total_completion_tokens\"] = \"\"\n",
    "    print(method)\n",
    "    counter = 0\n",
    "    for index, row in anno.iterrows():\n",
    "        print(index)\n",
    "        filtered = rank_and_filter(row[\"gui_indexes\"], row[\"query\"], retrieve=20, method=method, reasoning=True, ranking_type=ranking_type)\n",
    "        scores = []\n",
    "        reasonings = []\n",
    "        errors = []\n",
    "        responses = []\n",
    "        prompt_tokens = []\n",
    "        completion_tokens = []\n",
    "        total_prompt_tokens = 0\n",
    "        total_completion_tokens = 0\n",
    "        \n",
    "        for filt in filtered:\n",
    "            if filt[\"error\"] == 0:\n",
    "                scores.append(filt[\"score\"])\n",
    "                reasonings.append(filt[\"reasoning\"])\n",
    "                responses.append(filt[\"response\"])\n",
    "                errors.append(filt[\"error\"])\n",
    "            else:\n",
    "                scores.append(0)\n",
    "                reasonings.append(\"\")\n",
    "                responses.append(filt[\"full_response\"])\n",
    "                print(filt[\"full_response\"])\n",
    "                errors.append(filt[\"error\"])\n",
    "            try:\n",
    "                if method == \"image\":\n",
    "                    total_prompt_tokens += filt[\"full_response\"][\"usage\"][\"prompt_tokens\"]\n",
    "                    total_completion_tokens += filt[\"full_response\"][\"usage\"][\"completion_tokens\"]\n",
    "                    prompt_tokens.append(filt[\"full_response\"][\"usage\"][\"prompt_tokens\"])\n",
    "                    completion_tokens.append(filt[\"full_response\"][\"usage\"][\"completion_tokens\"])\n",
    "                else:\n",
    "                    total_prompt_tokens += filt[\"full_response\"].usage.prompt_tokens\n",
    "                    total_completion_tokens += filt[\"full_response\"].usage.completion_tokens\n",
    "                    prompt_tokens.append(filt[\"full_response\"].usage.prompt_tokens)\n",
    "                    completion_tokens.append(filt[\"full_response\"].usage.completion_tokens)\n",
    "            except Exception as e:\n",
    "                print(\"Error!\", type(e), e)\n",
    "                print(filt)\n",
    "                prompt_tokens.append(0)\n",
    "                completion_tokens.append(0)\n",
    "        anno.at[index, method] = scores\n",
    "        anno.at[index, method + \"_reasonings\"] = reasonings\n",
    "        anno.at[index, method + \"_errors\"] = errors\n",
    "        anno.at[index, method + \"_responses\"] = responses\n",
    "        anno.at[index, method + \"_prompt_tokens\"] = prompt_tokens\n",
    "        anno.at[index, method + \"_completion_tokens\"] = completion_tokens\n",
    "        anno.at[index, method + \"_total_prompt_tokens\"] = total_prompt_tokens\n",
    "        anno.at[index, method + \"_total_completion_tokens\"] = total_completion_tokens\n",
    "        if index >= 1:\n",
    "            break\n",
    "        counter += 1\n",
    "anno.to_csv(data_path + \"goldstandard_one_to_ten_image_anno.csv\")\n",
    "anno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno.to_csv(data_path + \"goldstandard_three_level_v2_image_anno.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reranking_anno = goldstandard_csv.copy()\n",
    "\n",
    "methods = [\"image\"]\n",
    "temp = 0.25\n",
    "for method in methods:\n",
    "    reranking_anno[method] = \"\"\n",
    "    reranking_anno[method + \"_reasoning\"] = \"\"\n",
    "    reranking_anno[method + \"_response\"] = \"\"\n",
    "    reranking_anno[method + \"_id_mappings\"] = \"\"\n",
    "    reranking_anno[method + \"_ranked_unmapped\"] = \"\"\n",
    "    reranking_anno[method + \"_full_response\"] = \"\"\n",
    "    reranking_anno[method + \"_prompt_tokens\"] = \"\"\n",
    "    reranking_anno[method + \"_completion_tokens\"] = \"\"\n",
    "    reranking_anno[method + \"_error\"] = \"\"\n",
    "    \n",
    "    print(method)\n",
    "    counter = 0\n",
    "    for index, row in reranking_anno.iterrows():\n",
    "        print(index)\n",
    "        no_error = False\n",
    "        reranked = rank_and_rerank(row[\"gui_indexes\"], row[\"query\"], retrieve=len(row[\"gui_indexes\"]), method=method, reasoning=True, temp=temp)\n",
    "        if reranked[\"error\"] == 0:\n",
    "            reranked[\"ranked\"] = list(map(int, reranked[\"ranked\"]))\n",
    "            if set(row[\"gui_indexes\"]) == set(reranked[\"ranked\"]) and len(reranked[\"ranked\"]) == len(row[\"gui_indexes\"]):\n",
    "                no_error = True\n",
    "            else:\n",
    "                print(\"Error!\")\n",
    "                print(set(row[\"gui_indexes\"]).difference(set(reranked[\"ranked\"])))\n",
    "                print(len(reranked[\"ranked\"]))\n",
    "        try:\n",
    "            reranking_anno.at[index, method + \"_full_response\"] = reranked[\"full_response\"]\n",
    "            if method == \"image\":\n",
    "                reranking_anno.at[index, method + \"_prompt_tokens\"] = reranked[\"full_response\"][\"usage\"][\"prompt_tokens\"]\n",
    "                reranking_anno.at[index, method + \"_completion_tokens\"] = reranked[\"full_response\"][\"usage\"][\"completion_tokens\"]\n",
    "            else:\n",
    "                reranking_anno.at[index, method + \"_prompt_tokens\"] = int(reranked[\"full_response\"].usage.prompt_tokens)\n",
    "                reranking_anno.at[index, method + \"_completion_tokens\"] = int(reranked[\"full_response\"].usage.completion_tokens)\n",
    "            reranking_anno.at[index, method] = reranked[\"ranked\"]\n",
    "            reranking_anno.at[index, method + \"_reasoning\"] = reranked[\"reasoning\"]\n",
    "            reranking_anno.at[index, method + \"_response\"] = reranked[\"response\"]\n",
    "            reranking_anno.at[index, method + \"_id_mappings\"] = reranked[\"id_mappings\"]\n",
    "            reranking_anno.at[index, method + \"_ranked_unmapped\"] = reranked[\"ranked_unmapped\"]\n",
    "            if no_error:\n",
    "                reranking_anno.at[index, method + \"_error\"] = reranked[\"error\"]\n",
    "            else:\n",
    "                reranking_anno.at[index, method + \"_error\"] = 1\n",
    "        except Exception as e:\n",
    "            print(type(e), e)\n",
    "            reranking_anno.at[index, method + \"_error\"] = 1       \n",
    "reranking_anno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reranking_anno = pd.read_csv(data_path + \"goldstandard_reranking_anno_image_run1_fix.csv\")\n",
    "missing_uis = []\n",
    "for index, row in reranking_anno.iterrows():\n",
    "    if row[\"image_error\"] == 1 and row[\"image\"] != \"\":\n",
    "        try:\n",
    "            print()\n",
    "            row[\"gui_indexes\"] = list(sorted(literal_eval(row[\"gui_indexes\"])))\n",
    "            row[\"image\"] = list(sorted(literal_eval(row[\"image\"])))\n",
    "            \n",
    "            print(row[\"gui_indexes\"])\n",
    "            print(row[\"image\"])\n",
    "            print(len(set(row[\"gui_indexes\"]).difference(set(row[\"image\"]))))\n",
    "            for diff in set(row[\"gui_indexes\"]).difference(set(row[\"image\"])):\n",
    "                print(diff)\n",
    "                print(str(diff) in row[\"image_reasoning\"])\n",
    "                print(diff in row[\"image\"])\n",
    "                print(diff in row[\"gui_indexes\"])\n",
    "                missing_uis.append(diff)\n",
    "            print(row[\"image_reasoning\"])\n",
    "            #show_images(row[\"gui_indexes\"], rico_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(row[\"image\"])\n",
    "print()\n",
    "print(\"MISSING\")\n",
    "print(missing_uis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Annotate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(data_path + \"dataset_test_k_top_20.csv\")\n",
    "test_dataset[\"rico_ranking\"] = test_dataset[\"rico_ranking\"].apply(literal_eval)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary/One to ten annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "method = \"image\"\n",
    "ranking_types = [\"binary\", \"one_to_ten\"]\n",
    "\n",
    "for ranking_type in ranking_types:\n",
    "    print(ranking_type)\n",
    "    test_dataset[ranking_type + \"_annotation\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_reasonings\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_full_responses\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_prompt_tokens\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_completion_tokens\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_total_prompt_tokens\"] = \"\"\n",
    "    test_dataset[ranking_type + \"_total_completion_tokens\"] = \"\"\n",
    "    for index, row in test_dataset.iterrows():\n",
    "        print(index)\n",
    "        filtered = rank_and_filter(row[\"rico_ranking\"], row[\"Descriptions\"], retrieve=len(row[\"rico_ranking\"]), method=method, reasoning=True, ranking_type=ranking_type)\n",
    "        scores = []\n",
    "        reasonings = []\n",
    "        full_responses = []\n",
    "        prompt_tokens = []\n",
    "        completion_tokens = []\n",
    "        total_prompt_tokens = 0\n",
    "        total_completion_tokens = 0\n",
    "        for filt in filtered:\n",
    "            if filt[\"error\"] == 0:\n",
    "                scores.append(filt[\"score\"])\n",
    "                reasonings.append(filt[\"reasoning\"])\n",
    "                full_response = filt[\"full_response\"]\n",
    "                full_responses.append(full_response)\n",
    "                prompt_tokens.append(full_response[\"usage\"][\"prompt_tokens\"])\n",
    "                completion_tokens.append(full_response[\"usage\"][\"completion_tokens\"])\n",
    "                total_prompt_tokens += full_response[\"usage\"][\"prompt_tokens\"]\n",
    "                total_completion_tokens += full_response[\"usage\"][\"completion_tokens\"]\n",
    "            else:\n",
    "                scores.append(0)\n",
    "                reasonings.append(\"\")\n",
    "                try:\n",
    "                    full_response = filt[\"full_response\"]\n",
    "                    full_responses.append(full_response)\n",
    "                    prompt_tokens.append(full_response[\"usage\"][\"prompt_tokens\"])\n",
    "                    completion_tokens.append(full_response[\"usage\"][\"completion_tokens\"])\n",
    "                    total_prompt_tokens += full_response[\"usage\"][\"prompt_tokens\"]\n",
    "                    total_completion_tokens += full_response[\"usage\"][\"completion_tokens\"]\n",
    "                except Exception as e:\n",
    "                    print(type(e), e)\n",
    "                    full_responses.append(\"Error\")\n",
    "                    prompt_tokens.append(0)\n",
    "                    completion_tokens.append(0)\n",
    "                    total_prompt_tokens += 0\n",
    "                    total_completion_tokens += 0\n",
    "\n",
    "        test_dataset.at[index, ranking_type + \"_annotation\"] = str(scores)\n",
    "        test_dataset.at[index, ranking_type + \"_reasonings\"] = str(reasonings)\n",
    "        test_dataset.at[index, ranking_type + \"_full_responses\"] = full_responses\n",
    "        test_dataset.at[index, ranking_type + \"_prompt_tokens\"] = str(prompt_tokens)\n",
    "        test_dataset.at[index, ranking_type + \"_completion_tokens\"] = str(completion_tokens)\n",
    "        test_dataset.at[index, ranking_type + \"_total_prompt_tokens\"] = total_prompt_tokens\n",
    "        test_dataset.at[index, ranking_type + \"_total_completion_tokens\"] = total_completion_tokens\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ranking_type in ranking_types:\n",
    "    print(ranking_type)\n",
    "    test_dataset[ranking_type + \"_annotation\"] = test_dataset[ranking_type + \"_annotation\"].apply(literal_eval)\n",
    "    test_dataset[ranking_type + \"_reasonings\"] = test_dataset[ranking_type + \"_reasonings\"].apply(literal_eval)\n",
    "    test_dataset[ranking_type + \"_prompt_tokens\"] = test_dataset[ranking_type + \"_prompt_tokens\"].apply(literal_eval)\n",
    "    test_dataset[ranking_type + \"_completion_tokens\"] = test_dataset[ranking_type + \"_completion_tokens\"].apply(literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.to_csv(data_path + \"dataset_test_k_top_20_binary_one_to_ten_annotated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
